{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 基于 Gemma + Milvus 实现多模态 RAG 系统\n",
        "\n",
        "本练习展示了如何实现一个多模态的检索增强生成（RAG）系统。在检索增强生成中，除了输入提示外，还会使用外部信息源来生成响应。在多模态场景中，最常见的用例之一是将图像纳入响应生成过程。\n",
        "\n",
        "<br>\n",
        "\n",
        "本练习通过使用包含图像、文本和表格的 PDF 文件来实现多模态 RAG 系统。这个 PDF 文件就是前面提到的 RAG 定义中的外部信息源。一旦系统设置完成，模型将能够根据所提供的 PDF 文件中的图像、文本和表格生成响应。\n",
        "\n",
        "以下是本练习涵盖的主题列表：\n",
        "\n",
        "- 安装依赖项<br>\n",
        "- 处理 PDF<br>\n",
        "- 生成多模态嵌入<br>\n",
        "- 创建向量数据库<br>\n",
        "- 生成 RAG 响应<br>\n",
        "- 测试 RAG 工作流<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 安装依赖项"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 处理 PDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pymupdf\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import base64\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "def pdf2imgs(pdf_path, pdf_pages_dir=\"data/pdf_pages\"):\n",
        "    \"\"\"\n",
        "    Convert a PDF file to individual PNG images for each page.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): The path to the PDF file.\n",
        "        pdf_pages_dir (str, optional): The directory to save the PNG images. Defaults to \"data/pdf_pages\".\n",
        "\n",
        "    Returns:\n",
        "        str: The path to the directory containing the PNG images.\n",
        "    \"\"\"\n",
        "    import pypdfium2 as pdfium\n",
        "    # Open the PDF document\n",
        "    pdf = pdfium.PdfDocument(pdf_path)\n",
        "\n",
        "    # Create the directory to save the PNG images if it doesn't exist\n",
        "    os.makedirs(pdf_pages_dir, exist_ok=True)\n",
        "\n",
        "    # Get the resolution of the first page to determine the scale factor\n",
        "    resolution = pdf.get_page(0).render().to_numpy().shape\n",
        "    scale = 1 if max(resolution) >= 1620 else 300 / 72  # Scale factor based on resolution\n",
        "\n",
        "    # Get the number of pages in the PDF\n",
        "    n_pages = len(pdf)\n",
        "\n",
        "    # Loop through each page and save as a PNG image\n",
        "    for page_number in range(n_pages):\n",
        "        page = pdf.get_page(page_number)\n",
        "        pil_image = page.render(\n",
        "            scale=scale,\n",
        "            rotation=0,\n",
        "            crop=(0, 0, 0, 0),\n",
        "            may_draw_forms=False,\n",
        "            fill_color=(255, 255, 255, 255),\n",
        "            draw_annots=False,\n",
        "            grayscale=False,\n",
        "        ).to_pil()\n",
        "        image_path = os.path.join(pdf_pages_dir, f\"{str(file_path).split('/')[-1]}_page_{page_number:03d}.png\")\n",
        "        pil_image.save(image_path)\n",
        "\n",
        "    return pdf_pages_dir\n",
        "\n",
        "\n",
        "def process_pdf(file_path: str) -> bool:\n",
        "    \"\"\"\n",
        "    Process PDF file:\n",
        "        1. 抽取文本，然后使用langchain的RecursiveCharacterTextSplitter对文本进行分块，然后保存到text文件夹\n",
        "        2. 抽取文章的图片，保存到iamges文件夹\n",
        "        3. 将每页文档转换为图片，保存到page_images文件夹\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Define the directories to store the extracted text, images and page images from each page\n",
        "        filename = Path(file_path.split(\"/\")[-1]) \n",
        "        data_dir = Path(\"./data/\")\n",
        "        doc = pymupdf.open(file_path)\n",
        "        num_pages = len(doc)\n",
        "        image_save_dir = data_dir / \"images\"\n",
        "        text_save_dir =data_dir / \"text\"\n",
        "        page_images_save_dir = data_dir / \"page_images\"\n",
        "\n",
        "        # Chunk the text for effective retrieval\n",
        "        chunk_size = 1000\n",
        "        overlap=100\n",
        "        \n",
        "        items = []\n",
        "        # Process all pages of the PDF\n",
        "        for page_num in tqdm(range(num_pages), desc=\"Processing PDF pages\"):\n",
        "            page = page = doc[page_num]\n",
        "            text = page.get_text()\n",
        "            \n",
        "            # # Process chunks with overlap\n",
        "            # chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size-overlap)]\n",
        "            # !pip install -qU langchain-text-splitters \n",
        "\n",
        "            # Process chunks with RecursiveCharaterTextSplitter\n",
        "            text_splitter = RecursiveCharacterTextSplitter(\n",
        "                separators=[\n",
        "                    \"\\n\\n\",\n",
        "                    \"\\n\",\n",
        "                    \" \",\n",
        "                    \".\",\n",
        "                    \",\",\n",
        "                    \"。\",\n",
        "                    \"．\",\n",
        "                    \"、\",\n",
        "                    \"，\",\n",
        "                    \"\\u200b\",  # 零宽度空格\n",
        "                ],\n",
        "                chunk_size=chunk_size,\n",
        "                chunk_overlap=overlap\n",
        "            )\n",
        "            chunks = text_splitter.split_text(text)\n",
        "\n",
        "            # Generate an item to add to items\n",
        "            for i,chunk in enumerate(chunks):\n",
        "                text_file_name = f\"{text_save_dir}/{filename}_text_{page_num}_{i}.txt\"\n",
        "                # If the text folder doesn't exist, create one\n",
        "                os.makedirs(text_save_dir, exist_ok=True)\n",
        "                with open(text_file_name, 'w') as f:\n",
        "                    f.write(chunk)\n",
        "                \n",
        "                item={}\n",
        "                item[\"page\"] = page_num\n",
        "                item[\"type\"] = \"text\"\n",
        "                item[\"text\"] = chunk\n",
        "                item[\"path\"] = text_file_name\n",
        "                items.append(item)\n",
        "            \n",
        "            \n",
        "            # Get all the images in the current page\n",
        "            images = page.get_images()\n",
        "            for idx, image in enumerate(images):        \n",
        "                # Extract the image data\n",
        "                xref = image[0]\n",
        "                pix = pymupdf.Pixmap(doc, xref)\n",
        "                pix.tobytes(\"png\")\n",
        "                # Create the image_name that includes the image path\n",
        "                image_name = f\"{image_save_dir}/{filename}_image_{page_num}_{idx}_{xref}.png\"\n",
        "                # If the image folder doesn't exist, create one\n",
        "                os.makedirs(image_save_dir, exist_ok=True)\n",
        "                # Save the image\n",
        "                pix.save(image_name)\n",
        "                \n",
        "                # Produce base64 string\n",
        "                with open(image_name, 'rb') as f:\n",
        "                    image = base64.b64encode(f.read()).decode('utf8')\n",
        "                \n",
        "                item={}\n",
        "                item[\"page\"] = page_num\n",
        "                item[\"type\"] = \"image\"\n",
        "                item[\"path\"] = image_name\n",
        "                item[\"image\"] = image\n",
        "                items.append(item)\n",
        "\n",
        "        # Save pdf pages as images\n",
        "        try:\n",
        "            page_images_save_dir = pdf2imgs(file_path, page_images_save_dir)\n",
        "        except Exception as e:\n",
        "            print(f\"Error in processing page image saving.\")\n",
        "\n",
        "        for page_num in range(num_pages):\n",
        "            page_path = os.path.join(page_images_save_dir,  f\"{str(file_path).split('/')[-1]}_page_{page_num:03d}.png\")\n",
        "            # Produce base64 string\n",
        "            with open(page_path, 'rb') as f:\n",
        "                page_image = base64.b64encode(f.read()).decode('utf8')\n",
        "            \n",
        "            item = {}\n",
        "            item[\"page\"] = page_num\n",
        "            item[\"type\"] = \"page\"\n",
        "            item[\"path\"] = page_path\n",
        "            item[\"image\"] = page_image\n",
        "            items.append(item)\n",
        "            \n",
        "        return items\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing PDF: {str(e)}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing PDF pages: 100%|██████████| 11/11 [00:00<00:00, 61.68it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'page': 0,\n",
              "  'type': 'text',\n",
              "  'text': 'Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly',\n",
              "  'path': 'data/text/transformers_paper.pdf_text_0_0.txt'},\n",
              " {'page': 0,\n",
              "  'type': 'text',\n",
              "  'text': 'be superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].',\n",
              "  'path': 'data/text/transformers_paper.pdf_text_0_1.txt'},\n",
              " {'page': 0,\n",
              "  'type': 'text',\n",
              "  'text': 'architectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.',\n",
              "  'path': 'data/text/transformers_paper.pdf_text_0_2.txt'}]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "file_path = \"./data/transformers_paper.pdf\"\n",
        "items = process_pdf(file_path)\n",
        "items[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 生成多模态嵌入"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 加载 Visualized BGE 嵌入模型\n",
        "\n",
        "我们将使用 Visualized BGE 模型来生成图像和文本的嵌入向量。你需要下载预训练的权重并构建编码器。\n",
        "```\n",
        "!git clone https://github.com/FlagOpen/FlagEmbedding.git\n",
        "!cd FlagEmbedding/research/visual_bge\n",
        "!pip install -e .\n",
        "!wget https://huggingface.co/BAAI/bge-visualized/blob/main/Visualized_m3.pth\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path \n",
        "\n",
        "\n",
        "class BGEVisualizedEncoder:\n",
        "    \"\"\"Manager for BGE Visualized encoders with language support.\n",
        "    \n",
        "    Supported languages:\n",
        "        - 'en': English model (bge-base-en-v1.5)\n",
        "        - 'multilingual': Multilingual model (bge-m3)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize English and multilingual encoders.\"\"\"\n",
        "        try:\n",
        "            import sys\n",
        "            sys.path.append('./FlagEmbedding/research/visual_bge')\n",
        "            from visual_bge.modeling import Visualized_BGE\n",
        "            \n",
        "            self.en_encoder = Visualized_BGE(\n",
        "                model_name_bge=\"BAAI/bge-base-en-v1.5\",\n",
        "                model_weight=\"./models/Visualized_base_en_v1.5.pth\"\n",
        "            ).eval()\n",
        "            \n",
        "            self.m3_encoder = Visualized_BGE(\n",
        "                model_name_bge=\"BAAI/bge-m3\",\n",
        "                model_weight=\"./models/Visualized_m3.pth\"\n",
        "            ).eval()\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to initialize encoders: {e}\")\n",
        "\n",
        "    def get_encoder(self, language=\"en\"):\n",
        "        \"\"\"Get appropriate encoder based on language.\n",
        "        \n",
        "        Args:\n",
        "            language (str): Language option ('en' or 'multilingual')\n",
        "            \n",
        "        Returns:\n",
        "            Visualized_BGE: Initialized encoder model\n",
        "        \"\"\"\n",
        "        return self.en_encoder if language == \"en\" else self.m3_encoder\n",
        "\n",
        "\n",
        "def generate_bge_visualized_embeddings(encoder, image_path=None, text=None):\n",
        "    \"\"\"Generate embeddings using provided encoder.\n",
        "    \n",
        "    Args:\n",
        "        encoder: Initialized Visualized_BGE model\n",
        "        image_path (str, optional): Path to input image\n",
        "        text (str, optional): Text to encode with image\n",
        "        \n",
        "    Returns:\n",
        "        list: Generated embeddings\n",
        "        \n",
        "    Raises:\n",
        "        ValueError: If image_path is not provided\n",
        "    \"\"\"\n",
        "    \n",
        "    if not image_path and not text:\n",
        "        raise ValueError(\"Image path or text must be provided\")\n",
        "        \n",
        "    if image_path:\n",
        "        if not Path(image_path).exists():\n",
        "            raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
        "            \n",
        "    try:\n",
        "        if image_path and text:                \n",
        "            return encoder.encode(image=image_path, text=text).tolist()[0]\n",
        "        if text:\n",
        "            return encoder.encode(text=text).tolist()[0]\n",
        "\n",
        "        return encoder.encode(image=image_path).tolist()[0]\n",
        "    \n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Encoding failed: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating embeddings: 100%|██████████| 54/54 [00:12<00:00,  4.42it/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "bge_encoder = BGEVisualizedEncoder()\n",
        "encoder = bge_encoder.get_encoder(language=\"multilingual\")  # or \"en\"\n",
        "\n",
        "for item in tqdm(items, \"Generating embeddings\"):\n",
        "    if item['type'] == 'text':\n",
        "        item['vector'] = generate_bge_visualized_embeddings(encoder=encoder, text=item['text'])\n",
        "        \n",
        "    else:\n",
        "        item['vector'] = generate_bge_visualized_embeddings(encoder=encoder, image_path=item['path'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 创建向量数据库"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 将嵌入向量插入 Milvus\n",
        "\n",
        "接下来，我们将使用 Milvus 向量数据库来存储图像的路径和它们的嵌入向量。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'insert_count': 54, 'ids': [456939301895143424, 456939301895143425, 456939301895143426, 456939301895143427, 456939301895143428, 456939301895143429, 456939301895143430, 456939301895143431, 456939301895143432, 456939301895143433, 456939301895143434, 456939301895143435, 456939301895143436, 456939301895143437, 456939301895143438, 456939301895143439, 456939301895143440, 456939301895143441, 456939301895143442, 456939301895143443, 456939301895143444, 456939301895143445, 456939301895143446, 456939301895143447, 456939301895143448, 456939301895143449, 456939301895143450, 456939301895143451, 456939301895143452, 456939301895143453, 456939301895143454, 456939301895143455, 456939301895143456, 456939301895143457, 456939301895143458, 456939301895143459, 456939301895143460, 456939301895143461, 456939301895143462, 456939301895143463, 456939301895143464, 456939301895143465, 456939301895143466, 456939301895143467, 456939301895143468, 456939301895143469, 456939301895143470, 456939301895143471, 456939301895143472, 456939301895143473, 456939301895143474, 456939301895143475, 456939301895143476, 456939301895143477], 'cost': 0}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pymilvus import MilvusClient\n",
        "\n",
        "# 设置嵌入向量的维度\n",
        "# dim = len(list(item.values())[0])\n",
        "dim = len(list(item.values())[-1])\n",
        "\n",
        "# 设置 Milvus 集合的名称\n",
        "collection_name = \"multimodal_rag_on_pdf\"\n",
        "\n",
        "# 连接到 Milvus 客户端\n",
        "# 这里我们使用本地的 Milvus 实例，你可以根据你的设置进行更改\n",
        "milvus_client = MilvusClient(uri='./db/multimodal_rag_milvus_project.db')\n",
        "\n",
        "# 创建 Milvus 集合\n",
        "if collection_name not in milvus_client.list_collections():\n",
        "    milvus_client.create_collection(\n",
        "        collection_name=collection_name,\n",
        "        auto_id=True,\n",
        "        dimension=dim,\n",
        "        enable_dynamic_field=True,\n",
        "    )\n",
        "\n",
        "# 将数据插入到集合中\n",
        "\n",
        "milvus_client.insert(\n",
        "    collection_name=collection_name,\n",
        "    data=items,\n",
        ")\n",
        "\n",
        "# # 加载已存在的集合（如果需要重新运行代码）\n",
        "# milvus_client.load_collection(collection_name)\n",
        "\n",
        "# # 检查集合状态\n",
        "# collection_stats = milvus_client.get_load_state(collection_name)\n",
        "# print(f\"Collection status: {collection_stats}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 生成RAG回复"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "def generate_rag_response(prompt, matched_items):\n",
        "    \n",
        "    # Create context\n",
        "    text_context = \"\"\n",
        "    image_context = []\n",
        "    \n",
        "    client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
        "\n",
        "    \n",
        "    \n",
        "    for item in matched_items:\n",
        "        if 'text' in item.keys(): \n",
        "            text_context += str(item[\"page\"]) + \". \" + item['text'] + \"\\n\"\n",
        "        else:\n",
        "            image_context.append(item['image'])\n",
        "    \n",
        "    final_prompt = f\"\"\"You are a helpful assistant for question answering.\n",
        "    The text context is relevant information retrieved.\n",
        "    The provided image(s) are relevant information retrieved.\n",
        "    \n",
        "    <context>\n",
        "    {text_context}\n",
        "    </context>\n",
        "    \n",
        "    Answer the following question using the relevant context and images.\n",
        "    \n",
        "    <question>\n",
        "    {prompt}\n",
        "    </question>\n",
        "    \n",
        "    Answer:\"\"\"\n",
        "    \n",
        "    if image_context:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gemma-3-27b-it\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\"type\": \"text\", \"text\": final_prompt},\n",
        "                        {\n",
        "                            \"type\": \"image_url\",\n",
        "                            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_context}\"},\n",
        "                        },\n",
        "                    ],\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=500 # 根据需要调整\n",
        "        )\n",
        "    else:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gemma-3-27b-it\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\"type\": \"text\", \"text\": final_prompt},\n",
        "                    ],\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=500 # 根据需要调整\n",
        "        )\n",
        "    result = response.choices[0].message.content\n",
        "\n",
        "    return result\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"How is the scaled-dot-product attention calculated?\"\n",
        "query_embedding = generate_bge_visualized_embeddings(encoder=encoder, text=query, image_path=None)\n",
        "\n",
        "\n",
        "# 在 Milvus 中执行搜索\n",
        "search_results = milvus_client.search(\n",
        "    collection_name=collection_name,\n",
        "    data=[query_embedding],\n",
        "    output_fields=['text', 'page', 'image'],\n",
        "    search_params={\"metric_type\": \"COSINE\", \"params\": {}},\n",
        "    limit=3 # 设置返回的搜索结果数量\n",
        ")[0]\n",
        "\n",
        "matched_items = [hit[\"entity\"] for hit in search_results]\n",
        "\n",
        "results= generate_rag_response(None, matched_items)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 未来改进方向\n",
        "\n",
        "1. 添加更多模态的支持（如视频、音频）\n",
        "2. 优化PDF图片，表哥抽取性能\n",
        "3. 优化向量检索策略\n",
        "3. 提升回答生成的质量\n",
        "4. 添加结果评估机制\n",
        "5. 利用FastAPI创建聊天UI\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 参考链接\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- BGE-Visualized 项目：https://github.com/FlagOpen/FlagEmbedding\n",
        "- Hugging Face 模型仓库：https://huggingface.co/BAAI/bge-visualized\n",
        "- Model Scope模型仓库：https://www.modelscope.cn/models/BAAI/bge-visualized/summary\n",
        "- Gemma 模型：https://ai.google.dev/gemma\n",
        "- Milvus 文档：https://milvus.io/docs"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "multimodal_rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
